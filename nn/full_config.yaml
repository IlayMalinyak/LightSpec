Data:
  # Basics
  log_dir: 'logs'
  exp_num: spec_minmax_targets
  light_model_name: "DoubleInputRegressor"
  spec_model_name: "MultiTaskRegressor"
  combined_model_name: "MultiEncoder"
  # Data
  dataset: "LightSpecDataset"
  data_dir: '/mnt/walkure_public/users/ilaykamai/npy'
  spectra_dir: '/mnt/walkure_public/users/ilaykamai/lamost'
  batch_size: 16
  num_epochs: 1000
  max_len_spectra: 4096
  max_days_lc: 720
  max_len_lc: 34560
  lc_freq: 0.0208
  continuum_norm: True
  meta_columns_lightspec: ['Teff', 'Mstar', 'RUWE']
  meta_columns_lc: []
  meta_columns_spec: []
  meta_columns_finetune: []
  meta_columns_simulation: []
  target_norm: 'minmax'
  test_run: False
  create_umap: False
  load_checkpoint: False
  masked_transform: False
  use_acf: False
  use_fft: False
  scale_flux: False
  combined_embed: False
  dim_lc: 4
  alpha: 0.9
  pred_coeff_val: None
  approach: "multitask"
  checkpoint_path: ''

DoubleInputRegressor_lc:
  encoder_only: True
  stacked_input: True
  in_channels: 2
  load_checkpoint: False
  output_dim: 1
  num_quantiles: 5
  dropout_p: 0.3  
  checkpoint_path: ''

CNNEncoder_lc:
  # Model
  in_channels: 2
  num_layers: 6
  stride: 1
  encoder_dims: [32,64,128,256,512]
  kernel_size: 3
  dropout_p: 0.3
  output_dim: 2
  beta: 1
  load_checkpoint: False
  checkpoint_num: 1
  activation: "sine"
  sine_w0: 1.0
  avg_output: True
  checkpoint_path: ''


AstroConformer_lc:
  # Model
  in_channels: 2
  encoder: ["mhsa_pro", "conv"]
  timeshift: false
  num_layers: 5
  num_decoder_layers: 6
  stride: 20
  encoder_dim: 512
  decoder_dim: 128
  num_heads: 8
  kernel_size: 3
  dropout_p: 0.3
  output_dim: 3
  encoder_only: True
  norm: "postnorm"
  deepnorm: True
  beta: 0.913    # Calculated as (num_layers/6)^(-0.5) for 5 layers
  load_checkpoint: False
  spec_checkpoint_path: ''


AstroEncoderDecoder:
  # Model
  in_channels: 1
  encoder: ["mhsa_pro", "conv", "mhsa_pro"]
  decoder: ["mhsa_decoder",  "conv", "mhsa_decoder"]
  timeshift: false
  num_layers: 6
  num_decoder_layers: 6
  stride: 2
  encoder_dim: 512
  decoder_dim: 512
  num_heads: 4
  kernel_size: 3
  dropout_p: 0.2
  output_dim: 3
  norm: "postnorm"
  load_spec_checkpoint: False
  spec_checkpoint_path: ''

MultiEncoder_lc:
  # Model
  in_channels: 2
  num_layers: 8
  stride: 1
  encoder_dims: [32,64,128,256,512,2048]
  kernel_size: 3
  dropout_p: 0.3
  output_dim: 2
  beta: 1
  avg_output: True
  load_checkpoint: False
  checkpoint_num: 1
  activation: "sine"
  sine_w0: 1.0
  checkpoint_path: ''


Conformer_lc:
  encoder: ["mhsa_pro", "conv", "ffn"]
  timeshift: false
  num_layers: 6
  stride: 2
  encoder_dim: 2048
  num_heads: 8
  kernel_size: 3
  dropout_p: 0.2
  norm: "shortcut"

MultiTaskSimSiam_lc:
  in_channels: 2
  num_layers: 8
  stride: 1
  encoder_dims: [32, 64,128,256,512,2048]
  transformer_layers: 4
  kernel_size: 3
  dropout_p: 0.2
  avg_output: True
  output_dim: 1
  num_quantiles: 5
  beta: 1
  load_checkpoint: False
  checkpoint_num: 1
  activation: "sine"
  sine_w0: 1.0
  checkpoint_path: ""

MultiTaskRegressor_spec:
  backbone: 'cnn'
  in_channels: 1
  num_layers: 5
  stride: 1
  encoder_dims: [64,128,256,1024,2048]
  transformer_layers: 4
  kernel_size: 3
  dropout_p: 0.2
  avg_output: False
  output_dim: 3
  num_quantiles: 5
  beta: 1
  load_checkpoint: False
  checkpoint_num: 1
  activation: "silu"
  checkpoint_path: ""


Conformer_spec:
  encoder: ["mhsa_pro", "conv", "ffn"]
  timeshift: false
  num_layers: 8
  encoder_dim: 2048
  num_heads: 8
  kernel_size: 3
  dropout_p: 0.2
  norm: "postnorm"
  deepnorm: True
  beta: 0.866    # Calculated as (num_layers/6)^(-0.5) for 8 layers


MultiEncoder_combined:
  # Model
  in_channels: 3
  num_layers: 8
  stride: 1
  encoder_dims: [32,64,128,256,512,2048]
  kernel_size: 3
  dropout_p: 0.3
  output_dim: 2
  beta: 1
  avg_output: True
  load_checkpoint: False
  checkpoint_num: 1
  activation: "sine"
  sine_w0: 1.0
  checkpoint_path: ''


Conformer_combined:
  encoder: ["mhsa_pro", "conv", "ffn"]
  timeshift: false
  num_layers: 6
  stride: 2
  encoder_dim: 2048
  num_heads: 8
  kernel_size: 3
  dropout_p: 0.2
  norm: "shortcut"

MultiEncoder_lightspec:
  # Model
  in_channels: 1
  num_layers: 6
  stride: 1
  encoder_dims: [32,64,128,256,512]
  kernel_size: 3
  dropout_p: 0.3
  output_dim: 2
  beta: 1
  load_checkpoint: False
  checkpoint_num: 1
  activation: "sine"
  sine_w0: 1.0
  avg_output: True
  checkpoint_path: ''

Conformer_lightspec:
  encoder: ["mhsa_pro", "conv"]
  timeshift: false
  num_layers: 4
  encoder_dim: 256
  num_heads: 8
  kernel_size: 3
  dropout_p: 0.2
  norm: "postnorm"

Transformer_lightspec:
  in_channels: 1
  num_layers: 2
  encoder_dim: 256
  dropout_p: 0.2
  num_heads: 8
  dropout: 0.0
  output_dim: 64
  num_quantiles: 1
  checkpoint_num: 1
  load_checkpoint: False
  deepnorm: True
  beta: 1.225    # Calculated as (num_layers/6)^(-0.5) for 2 layers

Transformer_jepa:
  in_channels: 1
  num_layers: 4
  encoder_dim: 256
  dropout_p: 0.2
  num_heads: 8
  dropout: 0.0
  output_dim: 256
  num_quantiles: 1
  checkpoint_num: 1
  load_checkpoint: False
  deepnorm: True
  beta: 0.866    # Calculated as (num_layers/6)^(-0.5) for 4 layers


Conformer_lightspec:
  encoder: ["mhsa_pro", "conv", "ffn"]
  timeshift: false
  num_layers: 4
  stride: 2
  encoder_dim: 256
  num_heads: 8
  kernel_size: 3
  dropout_p: 0.2
  output_dim: 256
  norm: "shortcut"

CNNEncoder:
  # Model
  in_channels: 1
  num_layers: 8
  stride: 1
  encoder_dims: [32,64,128,256,512]
  kernel_size: 3
  dropout_p: 0.2
  output_dim: 2
  beta: 1
  load_checkpoint: True
  avg_output: True
  checkpoint_num: 2
  activation: "sine"
  sine_w0: 1.0
  load_checkpoint: True
  checkpoint_path: '/data/lightSpec/logs/light_2024-11-27/CNNEncoder_lc_1.pth'

CNNEncoderDecoder_lc:
  # Model
  in_channels: 1
  num_layers: 8
  stride: 1
  encoder_dims: [32,64,128,256,512]
  kernel_size: 3
  dropout_p: 0.3
  output_dim: 2
  beta: 1
  load_checkpoint: True
  checkpoint_num: 1
  activation: "sine"
  sine_w0: 1.0
  avg_output: True
  checkpoint_path: '/data/lightSpec/logs/light_2025-02-04/CNNEncoderDecoder_lc_2.pth'



MoCo:
  # Model
  K: 4096
  m: 0.9
  T: 0.05
  hidden_dim: 512
  projection_dim: 64
  num_layers: 6
  shared_dim: 64
  freeze_lightcurve: False
  freeze_spectra: False
  freeze_combined: False
  bidirectional: True
  # deepnorm: True
  # beta: 1.0  # Default beta value

projector:
  in_dim: 256
  hidden_dim: 1024
  out_dim: 64

predictor:
  in_dim: 64
  hidden_dim: 128
  out_dim: 64
  w_dim: 3

reg_predictor:
  in_dim: 128
  hidden_dim: 512
  out_dim: 20
  w_dim: 0

loss:
  sim_coeff: 25
  std_coeff: 25 
  cov_coeff: 1


Tuner:
  in_dim: 128
  hidden_dim: 64
  out_dim: 1
  w_dim: 0

Test_Tuner:
  encoder_dim: 256
  in_dim: 128
  hidden_dim: 64
  output_dim: 1
  num_quantiles: 1
  w_dim: 0
  
MultiModalSimSiam:
  # Model
  input_dim: 512
  hidden_dim: 64
  projection_dim: 32
  output_dim: 32
  dropout: 0.2
  freeze_backbone: True

CNNBackbone:
  # Model
  in_channels: 1
  num_layers: 8
  stride: 1
  encoder_dims: [64,128,256,512]
  kernel_size: 3
  dropout_p: 0.3
  beta: 1
  avg_output: True
  activation: "silu"
  sine_w0: 1.0
  load_checkpoint: False

conv_args:
  in_channels: 1
  num_layers: 8
  stride: 1
  encoder_dims: [64,128,256,512]
  kernel_size: 3
  dropout_p: 0.3
  beta: 1
  avg_output: True
  activation: "silu"
  
Optimization:
  # Optimization
  max_lr:  5e-5
  weight_decay: 1e-6
  warmup_pct: 0.15
  steps_per_epoch: 3500
  momentum: 0.95
  nesterov: true
  optimizer: "adamw"
  quantiles: [0.1, 0.25, 0.5, 0.75, 0.9]

