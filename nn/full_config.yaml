Data:
  # Basics
  log_dir: '/data/lightSpec/logs'
  exp_num: lightspec_dual_former_6_latent_giants_finetune_age
  light_model_name: "DoubleInputRegressor"
  spec_model_name: "MultiTaskRegressor"
  combined_model_name: "MultiEncoder"
  # Data
  dataset: "LightSpecDataset"
  data_dir: '/data/lightPred/data'
  spectra_dir: '/data/lamost/data'
  batch_size: 8
  num_epochs: 1000
  max_len_spectra: 4096
  max_days_lc: 720
  max_len_lc: 34560
  lc_freq: 0.0208
  continuum_norm: True
  meta_columns_lightspec: ['Teff', 'Mstar', 'RUWE']
  meta_columns_lc: []
  meta_columns_spec: []
  meta_columns_finetune: []
  meta_columns_simulation: []
  prediction_labels_lightspec: ["Teff",  "logg", "Prot",  "FeH", "Rstar",  "RUWE"]
  prediction_labels_lc: ['Teff', 'logg', 'Prot']
  prediction_labels_spec: ['Teff', 'logg', 'Prot']
  prediction_labels_finetune: [final_age_norm, 'age_error_rel']
  prediction_labels_simulation: ['Period']
  target_norm: 'solar'
  test_run: False
  create_umap: False
  load_checkpoint: True
  masked_transform: False
  use_acf: True
  use_fft: True
  scale_flux: False
  combined_embed: False
  dim_lc: 4
  ssl_weight: 0.5
  alpha: 0.1
  freeze_backbone: False
  use_latent: True
  pred_coeff_val: 0
  approach: "dual_former"
  only_main_seq: False
  # checkpoint_path: '/data/lightSpec/logs/lightspec_2025-04-18/lightspec_dual_former_latent_full.pth'
  # checkpoint_path: '/data/lightSpec/logs/lightspec_2025-04-15/lightspec_dual_former_teff_logg_prot_duality_loss_freeze_backbone_latent_2.pth'
  # checkpoint_path: '/data/lightSpec/logs/lightspec_2025-04-19/lightspec_dual_former_latent_full_proj_covloss.pth'
  # checkpoint_path: '/data/lightSpec/logs/lightspec_2025-04-19/lightspec_dual_former_latent_full_proj_covloss_mainseq.pth'
  # checkpoint_path: '/data/lightSpec/logs/lightspec_2025-04-20/lightspec_dual_former_6_latent_proj_covloss_2.pth'
  checkpoint_path: '/data/lightSpec/logs/lightspec_2025-04-22/lightspec_dual_former_6_latent_proj_covloss_giants.pth'

DoubleInputRegressor_lc:
  encoder_only: True
  stacked_input: True
  in_channels: 2
  load_checkpoint: True
  output_dim: 1
  num_quantiles: 5
  dropout_p: 0.3  
  checkpoint_path: '/data/lightSpec/logs/light_2025-03-19/DoubleInputRegressor_lc_3_ssl.pth'

CNNEncoder_lc:
  # Model
  in_channels: 2
  num_layers: 6
  stride: 1
  encoder_dims: [32,64,128,256,512]
  kernel_size: 3
  dropout_p: 0.3
  output_dim: 2
  beta: 1
  load_checkpoint: False
  checkpoint_num: 1
  activation: "sine"
  sine_w0: 1.0
  avg_output: True
  checkpoint_path: '/data/lightSpec/logs/light_2024-11-27/CNNEncoder_lc_1.pth'


AstroConformer_lc:
  # Model
  in_channels: 2
  encoder: ["mhsa_pro", "conv"]
  timeshift: false
  num_layers: 5
  num_decoder_layers: 6
  stride: 20
  encoder_dim: 512
  decoder_dim: 128
  num_heads: 8
  kernel_size: 3
  dropout_p: 0.3
  output_dim: 3
  encoder_only: True
  norm: "postnorm"
  deepnorm: True
  beta: 0.913  
  load_checkpoint: False
  spec_checkpoint_path: '/data/lightSpec/logs/exp6/astroconf_spectra_3.pth'


AstroEncoderDecoder:
  # Model
  in_channels: 1
  encoder: ["mhsa_pro", "conv", "mhsa_pro"]
  decoder: ["mhsa_decoder",  "conv", "mhsa_decoder"]
  timeshift: false
  num_layers: 6
  num_decoder_layers: 6
  stride: 2
  encoder_dim: 512
  decoder_dim: 512
  num_heads: 4
  kernel_size: 3
  dropout_p: 0.2
  output_dim: 3
  norm: "postnorm"
  load_spec_checkpoint: False
  spec_checkpoint_path: '/data/lightSpec/logs/exp6/astroconf_spectra_3.pth'

MultiEncoder_lc:
  # Model
  in_channels: 2
  num_layers: 8
  stride: 1
  encoder_dims: [32,64,128,256,512,2048]
  kernel_size: 3
  dropout_p: 0.3
  output_dim: 2
  beta: 1
  avg_output: True
  load_checkpoint: True
  checkpoint_num: 1
  activation: "sine"
  sine_w0: 1.0
  checkpoint_path: '/data/lightSpec/logs/light_2025-02-11/MultiEncoder_lc_3.pth'


Conformer_lc:
  encoder: ["mhsa_pro", "conv", "ffn"]
  timeshift: false
  num_layers: 6
  stride: 2
  encoder_dim: 2048
  num_heads: 8
  kernel_size: 3
  dropout_p: 0.2
  norm: "shortcut"

MultiTaskSimSiam_lc:
  in_channels: 2
  num_layers: 8
  stride: 1
  encoder_dims: [32, 64,128,256,512,2048]
  transformer_layers: 4
  kernel_size: 3
  dropout_p: 0.2
  avg_output: True
  output_dim: 1
  num_quantiles: 5
  beta: 1
  load_checkpoint: True
  checkpoint_num: 1
  activation: "sine"
  sine_w0: 1.0
  checkpoint_path: "/data/lightSpec/logs/light_2025-03-03/MultiEncoder_lc_2.pth"

MultiTaskRegressor_spec:
  backbone: 'cnn'
  in_channels: 1
  num_layers: 5
  stride: 1
  encoder_dims: [64,128,256,1024,2048]
  transformer_layers: 4
  kernel_size: 3
  dropout_p: 0.2
  avg_output: False
  output_dim: 3
  num_quantiles: 5
  beta: 1
  load_checkpoint: True
  checkpoint_num: 1
  activation: "silu"
  checkpoint_path: "/data/lightSpec/logs/spec_decode2_2025-02-16/MultiTaskRegressor_spectra_decode_4.pth"
  # checkpoint_path: "/data/lightSpec/logs/spec_decode2_2025-04-01/spectra_decode_spec_minmax_targets.pth"


Conformer_spec:
  encoder: ["mhsa_pro", "conv", "ffn"]
  timeshift: false
  num_layers: 8
  encoder_dim: 2048
  num_heads: 8
  kernel_size: 3
  dropout_p: 0.2
  norm: "postnorm"
  deepnorm: True
  beta: 0.866    # Calculated as (num_layers/6)^(-0.5) for 8 layers


MultiEncoder_combined:
  # Model
  in_channels: 3
  num_layers: 8
  stride: 1
  encoder_dims: [32,64,128,256,512,2048]
  kernel_size: 3
  dropout_p: 0.3
  output_dim: 2
  beta: 1
  avg_output: True
  load_checkpoint: True
  checkpoint_num: 1
  activation: "sine"
  sine_w0: 1.0
  checkpoint_path: '/data/lightSpec/logs/combined_2025-02-12/lightspec_1.pth'


Conformer_combined:
  encoder: ["mhsa_pro", "conv", "ffn"]
  timeshift: false
  num_layers: 6
  stride: 2
  encoder_dim: 2048
  num_heads: 8
  kernel_size: 3
  dropout_p: 0.2
  norm: "shortcut"

MultiEncoder_lightspec:
  # Model
  in_channels: 1
  num_layers: 6
  stride: 1
  encoder_dims: [32,64,128,256,512]
  kernel_size: 3
  dropout_p: 0.3
  output_dim: 2
  beta: 1
  load_checkpoint: False
  checkpoint_num: 1
  activation: "sine"
  sine_w0: 1.0
  avg_output: True
  checkpoint_path: ''

Conformer_lightspec:
  encoder: ["mhsa_pro", "conv"]
  timeshift: false
  num_layers: 4
  encoder_dim: 256
  num_heads: 8
  kernel_size: 3
  dropout_p: 0.2
  norm: "postnorm"

Transformer_lightspec:
  in_channels: 1
  num_layers: 2
  encoder_dim: 64
  dropout_p: 0.2
  num_heads: 8
  dropout: 0.0
  output_dim: 64
  pooling_method: 'cls_token'
  num_quantiles: 1
  checkpoint_num: 1
  load_checkpoint: False
  deepnorm: True
  beta: 1.225    # Calculated as (num_layers/6)^(-0.5) for 2 layers

Transformer_jepa:
  in_channels: 1
  num_layers: 4
  encoder_dim: 512
  dropout_p: 0.2
  num_heads: 8
  dropout: 0.0
  output_dim: 256
  num_quantiles: 1
  checkpoint_num: 1
  load_checkpoint: False
  deepnorm: True
  beta: 0.866    # Calculated as (num_layers/6)^(-0.5) for 4 layers


Conformer_lightspec:
  encoder: ["mhsa_pro", "conv", "ffn"]
  timeshift: false
  num_layers: 4
  stride: 2
  encoder_dim: 256
  num_heads: 8
  kernel_size: 3
  dropout_p: 0.2
  output_dim: 256
  norm: "shortcut"

dual_former:
  input_dim: 1
  embed_dim: 256
  output_dim: 3
  num_heads: 8
  num_layers: 4
  ffn_dim: 2048
  dropout: 0.1
  attention_dropout: 0.1
  activation: "gelu"
  bidirectional: True
  norm_first: True
  use_positional_encoding: True
  max_seq_len: 5000
  pooling: "mean"  # Options: mean, max, cls, none
  use_prediction_head: False
  latent_dim: 0
  
    

CNNEncoder:
  # Model
  in_channels: 1
  num_layers: 8
  stride: 1
  encoder_dims: [32,64,128,256,512]
  kernel_size: 3
  dropout_p: 0.2
  output_dim: 2
  beta: 1
  load_checkpoint: True
  avg_output: True
  checkpoint_num: 2
  activation: "sine"
  sine_w0: 1.0
  load_checkpoint: True
  checkpoint_path: '/data/lightSpec/logs/light_2024-11-27/CNNEncoder_lc_1.pth'

CNNEncoderDecoder_lc:
  # Model
  in_channels: 1
  num_layers: 8
  stride: 1
  encoder_dims: [32,64,128,256,512]
  kernel_size: 3
  dropout_p: 0.3
  output_dim: 2
  beta: 1
  load_checkpoint: True
  checkpoint_num: 1
  activation: "sine"
  sine_w0: 1.0
  avg_output: True
  checkpoint_path: '/data/lightSpec/logs/light_2025-02-04/CNNEncoderDecoder_lc_2.pth'



MoCo:
  # Model
  K: 4096
  m: 0.9
  T: 0.05
  hidden_dim: 512
  projection_dim: 64
  num_layers: 6
  shared_dim: 64
  freeze_lightcurve: False
  freeze_spectra: False
  freeze_combined: False
  bidirectional: True
  # deepnorm: True
  # beta: 1.0  # Default beta value

projector:
  in_dim: 256
  hidden_dim: 1024
  out_dim: 64

predictor:
  in_dim: 64
  hidden_dim: 128
  out_dim: 64
  w_dim: 3

reg_predictor:
  in_dim: 128
  hidden_dim: 512
  out_dim: 20
  w_dim: 0

loss:
  sim_coeff: 25
  std_coeff: 25 
  cov_coeff: 0.1


Tuner:
  in_dim: 256
  hidden_dim: 256
  out_dim: 2
  w_dim: 0

Test_Tuner:
  encoder_dim: 256
  in_dim: 128
  hidden_dim: 64
  output_dim: 1
  num_quantiles: 1
  w_dim: 0
  
MultiModalSimSiam:
  # Model
  input_dim: 512
  hidden_dim: 64
  projection_dim: 32
  output_dim: 32
  dropout: 0.2
  freeze_backbone: True

CNNBackbone:
  # Model
  in_channels: 1
  num_layers: 8
  stride: 1
  encoder_dims: [64,128,256,512]
  kernel_size: 3
  dropout_p: 0.3
  beta: 1
  avg_output: True
  activation: "silu"
  sine_w0: 1.0
  load_checkpoint: False

conv_args:
  in_channels: 1
  num_layers: 8
  stride: 1
  encoder_dims: [64,128,256,512]
  kernel_size: 3
  dropout_p: 0.3
  beta: 1
  avg_output: True
  activation: "silu"
  
Optimization:
  # Optimization
  max_lr:  5e-5
  weight_decay: 1e-6
  warmup_pct: 0.15
  steps_per_epoch: 3500
  momentum: 0.95
  nesterov: true
  optimizer: "adamw"
  # quantiles: [0.1, 0.25, 0.5, 0.75, 0.9]
  quantiles: [0.5]

